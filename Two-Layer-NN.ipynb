{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
      "Epoch: 0 Loss: 2.303128710227561\n",
      "Epoch: 100 Loss: 0.9506855801160495\n",
      "Epoch: 200 Loss: 0.5032085995233072\n",
      "Epoch: 300 Loss: 0.4050727488695916\n",
      "Epoch: 400 Loss: 0.36285651466103924\n",
      "Epoch: 500 Loss: 0.3381541061379601\n",
      "Epoch: 600 Loss: 0.3208366637227774\n",
      "Epoch: 700 Loss: 0.307216747956839\n",
      "Epoch: 800 Loss: 0.2956246972406093\n",
      "Epoch: 900 Loss: 0.2852147570609682\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return e_x / e_x.sum(axis=1, keepdims=True)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "class TwoLayerNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.params = {\n",
    "            'W1': np.random.randn(input_size, hidden_size) * 0.01,\n",
    "            'b1': np.zeros((1, hidden_size)),\n",
    "            'W2': np.random.randn(hidden_size, output_size) * 0.01,\n",
    "            'b2': np.zeros((1, output_size))\n",
    "        }\n",
    "\n",
    "    def forward(self, X):\n",
    "        z1 = np.dot(X, self.params['W1']) + self.params['b1']\n",
    "        a1 = relu(z1)\n",
    "        z2 = np.dot(a1, self.params['W2']) + self.params['b2']\n",
    "        a2 = softmax(z2)\n",
    "        return a1, a2\n",
    "\n",
    "    def compute_loss(self, Y, output):\n",
    "        m = Y.shape[0]\n",
    "        log_likelihood = -np.log(output[range(m), Y])\n",
    "        loss = np.sum(log_likelihood) / m\n",
    "        return loss\n",
    "\n",
    "    def backward(self, X, Y, a1, output):\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        delta3 = output\n",
    "        delta3[range(m), Y] -= 1\n",
    "        dW2 = np.dot(a1.T, delta3) / m\n",
    "        db2 = np.sum(delta3, axis=0) / m\n",
    "\n",
    "        delta2 = np.dot(delta3, self.params['W2'].T) * relu_derivative(a1)\n",
    "        dW1 = np.dot(X.T, delta2) / m\n",
    "        db1 = np.sum(delta2, axis=0) / m\n",
    "\n",
    "        grads = {\n",
    "            'W1': dW1, 'b1': db1,\n",
    "            'W2': dW2, 'b2': db2\n",
    "        }\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def update_params(self, grads, learning_rate):\n",
    "        for key in self.params:\n",
    "            self.params[key] -= learning_rate * grads[key]\n",
    "\n",
    "    def train(self, X, Y, epochs, learning_rate):\n",
    "        for epoch in range(epochs):\n",
    "            a1, output = self.forward(X)\n",
    "            loss = self.compute_loss(Y, output)\n",
    "            grads = self.backward(X, Y, a1, output)\n",
    "            self.update_params(grads, learning_rate)\n",
    "            if epoch % 100 == 0:\n",
    "                print(\"Epoch:\", epoch, \"Loss:\", loss)\n",
    "\n",
    "# Load and preprocess MNIST data\n",
    "def load_mnist():\n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "    x_train = x_train.reshape(-1, 784).astype('float32') / 255  # Flatten and normalize\n",
    "    x_test = x_test.reshape(-1, 784).astype('float32') / 255\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_size = 784  # For MNIST dataset (28x28 images)\n",
    "    hidden_size = 128  # Size of the hidden layer\n",
    "    output_size = 10   # Number of classes in MNIST (0-9)\n",
    "\n",
    "    # Load MNIST data\n",
    "    X_train, Y_train, X_test, Y_test = load_mnist()\n",
    "\n",
    "    nn = TwoLayerNN(input_size, hidden_size, output_size)\n",
    "    nn.train(X_train, Y_train, epochs=1000, learning_rate=0.1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
